{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment classification of TV reviews using ReLU  \n",
    "Sentiment analysis of \"[Tatort](https://en.wikipedia.org/wiki/Tatort)\" Reviews.  \n",
    "Data source: Facebook comments pulled from weekly 'poll' postings on the 'dasErste' FB page.\n",
    "\n",
    "1 = positive review  \n",
    "2 = negative review\n",
    "\n",
    "## Import and review the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews:  2000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Evalu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>schlechteste ever ðŸ˜£</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6 absolut kein thema mehr</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ein sehr guter tatort (y), das franken-team so...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>absolute 6!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kann nicht mitreden. \"x-men: apocalypse\" auf s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Evalu\n",
       "0                                schlechteste ever ðŸ˜£      0\n",
       "1                          6 absolut kein thema mehr      0\n",
       "2  ein sehr guter tatort (y), das franken-team so...      1\n",
       "3                                        absolute 6!      0\n",
       "4  kann nicht mitreden. \"x-men: apocalypse\" auf s...      0"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "\n",
    "tatortreviews = pd.read_csv('data/tatort_reviews_labeled.csv')\n",
    "print(\"Number of reviews: \", len(tatortreviews))\n",
    "\n",
    "tatortreviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews = tatortreviews['Review']\n",
    "labels = tatortreviews['Evalu']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total word count:  6016\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "total_wordcount = Counter()\n",
    "\n",
    "for row in reviews:\n",
    "    total_wordcount.update(row.split(\" \"))\n",
    "    \n",
    "print(\"Total word count: \", len(total_wordcount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of words:  6000\n",
      "mal\n",
      "gut\n",
      "eine\n",
      "den\n",
      "es\n",
      "aber\n",
      "zu\n",
      "wieder\n",
      "1\n",
      "mit\n"
     ]
    }
   ],
   "source": [
    "vocabulary = sorted(total_wordcount, key=total_wordcount.get, reverse=True)[:6010]\n",
    "vocabulary = vocabulary[10:]\n",
    "    \n",
    "print(\"Num of words: \", len(vocabulary))\n",
    "for x in range(0, 10):\n",
    "    print(vocabulary[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "1442\n"
     ]
    }
   ],
   "source": [
    "wordindex = {word: i for i, word in enumerate(vocabulary)}\n",
    "\n",
    "print(wordindex.get('spannend'))\n",
    "print(wordindex.get('doof'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping text to vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_vector(text):\n",
    "    word_vector = np.zeros(len(vocabulary), dtype = np.int_)\n",
    "    for word in text.split(' '):\n",
    "        idx = wordindex.get(word, None)\n",
    "        if idx is None:\n",
    "            continue\n",
    "        else:\n",
    "            word_vector[idx] += 1\n",
    "    return np.array(word_vector)\n",
    " \n",
    "word_vectors = np.zeros((len(reviews), len(vocabulary)), dtype=np.int_)    \n",
    "for ii, text in enumerate(reviews):\n",
    "    word_vectors[ii] = text_to_vector(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into Train, Validation, Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(reviews, labels, test_size=0.1, random_state=666)\n",
    "\n",
    "#print(len(X_train))\n",
    "#print(len(Y_train))\n",
    "\n",
    "Y = labels\n",
    "records = len(labels)\n",
    "\n",
    "shuffle = np.arange(records)\n",
    "np.random.shuffle(shuffle)\n",
    "test_fraction = 0.9\n",
    "\n",
    "train_split, test_split = shuffle[:int(records*test_fraction)], shuffle[int(records*test_fraction):]\n",
    "X_train, Y_train = word_vectors[train_split], to_categorical(Y.values[train_split], 2)\n",
    "X_test, Y_test = word_vectors[test_split], to_categorical(Y.values[test_split], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_network_model():\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    #Inputs\n",
    "    net = tflearn.input_data([None, 6000])\n",
    "    \n",
    "    #Hidden layer\n",
    "    net = tflearn.fully_connected(net, 110, activation='ReLU')\n",
    "    \n",
    "    #Output Layer\n",
    "    net = tflearn.fully_connected(net, 2, activation='softmax')\n",
    "    net = tflearn.regression(net, optimizer='sgd', learning_rate=0.1, loss='categorical_crossentropy')\n",
    "    \n",
    "    model = tflearn.DNN(net)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Now, build it!\n",
    "model = build_network_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1039  | total loss: \u001b[1m\u001b[32m0.09192\u001b[0m\u001b[0m | time: 0.198s\n",
      "| SGD | epoch: 040 | loss: 0.09192 - acc: 0.9885 -- iter: 1600/1638\n",
      "Training Step: 1040  | total loss: \u001b[1m\u001b[32m0.09017\u001b[0m\u001b[0m | time: 1.211s\n",
      "| SGD | epoch: 040 | loss: 0.09017 - acc: 0.9896 | val_loss: 0.34819 - val_acc: 0.8704 -- iter: 1638/1638\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, validation_set = 0.09, show_metric=True, batch_size=64, n_epoch=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  [ 0.835  0.835]\n"
     ]
    }
   ],
   "source": [
    "predictions = (np.array(model.predict(X_test)) >= 0.5).astype(np.int_)\n",
    "test_accuracy = np.mean(predictions == Y_test, axis=0)\n",
    "print(\"Test accuracy: \", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06704208999872208, 0.9329578876495361]\n",
      "Positiv!\n"
     ]
    }
   ],
   "source": [
    "# Check with a custom sentence.\n",
    "# res = the result of the prediction, containing the positive and negative probability.\n",
    "res = model.predict([text_to_vector(\"ging so, aber die schauspieler waren gut\")])[0]\n",
    "print(res)\n",
    "if(res[1] > res[0]):\n",
    "    print('Positiv!')\n",
    "else:\n",
    "    print('Negativ!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}